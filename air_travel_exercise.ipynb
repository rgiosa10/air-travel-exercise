{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Write a data pipeline that ingests the source data into a **fact** called `air_travel_passengers` with two supporting **dimensions** as `airlines` and `airports`.\n",
    "\n",
    "To load the dimension tables, lookup additional columns from the supporting files: `global_airlines.csv` and `global_airports.csv`\n",
    "\n",
    "<br>\n",
    "\n",
    "Your data pipeline should look similar to:\n",
    "\n",
    "<img src=\"./imgs/dm_air_travel_exercise.jpg\" alt=\"Air Travel Pipeline\" width=\"700\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "Your pipeline must meet the following requirements:\n",
    "\n",
    "1. _airlines_ dimension:\n",
    "    - Looking up additional airline columns such as iata and icao codes, callsign, and country\n",
    "    - Generate a new airline_id by **hashing** the airline name\n",
    "1. _airports_ dimension:\n",
    "    - Using the airport iata code, look up additional column such as: airport lat/lon, icao code, and timezone information\n",
    "    - Set the iata code as the airport_id column\n",
    "1. _air\\_travel\\_passengers_ fact:\n",
    "    - Look up both airline_id and airport_id from their dimension tables\n",
    "    - Add a new column called _report\\_date_ set to the 1st of the report month/year (as date data type)\n",
    "    - Create a fact_id by hashing a **composite key** of: airline name, src, dest, year, and month\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Model\n",
    "\n",
    "Using draw.io, create a data model of your target tables. You must show at least three final tables: `air_travel_passengers`, `airlines`, and `airports`\n",
    "\n",
    "See data model below:\n",
    "\n",
    "<img src=\"./imgs/us_monthly_air_passangers.drawio.png\" alt=\"Air Travel Pipeline\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline\n",
    "\n",
    "Develope your pipeline code. We recommend breaking down the pipeline into the following sections (code cells):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.8 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/local/bin/python3.10 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import logging\n",
    "from google.cloud import bigquery\n",
    "from hashlib import md5\n",
    "from typing import List\n",
    "\n",
    "# **** SETUP ****\n",
    "\n",
    "# change to match your filesystem\n",
    "DATA_DIR = \"../data/air_travel/\"\n",
    "DEFAULT_RECEIPTS_FILE = os.path.join(DATA_DIR, \"us_monthly_air_passengers_sample.csv\")\n",
    "# change to match your gcloud project \n",
    "PROJECT_NAME = \"deb-01-371820\"\n",
    "DATASET_NAME = \"us_monthly_air_passengers\"\n",
    "\n",
    "# **** TABLE SCHEMAS ****\n",
    "\n",
    "TABLE_METADATA = {\n",
    "    'airlines': {\n",
    "        'table_name': 'airlines',\n",
    "        'schema': [\n",
    "            # indexes are written if only named in the schema\n",
    "            bigquery.SchemaField('airline_id', 'string', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('carrier_name', 'string', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('iata', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('icao', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('callsign', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('country', 'string', mode='NULLABLE'),\n",
    "        ],\n",
    "    },\n",
    "    'airports': {\n",
    "        'table_name': 'airports',\n",
    "        'schema': [\n",
    "            # indexes are written if only named in the schema\n",
    "            bigquery.SchemaField('airport_id', 'string', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('name', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('city', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('country', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('icao', 'string', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('latitude', 'float', mode='NULLABLE'),\n",
    "            bigquery.SchemaField('longitude', 'float', mode='NULLABLE'),\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "filename = DEFAULT_RECEIPTS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** SETUP LOGGING ****\n",
    "# setup logging and logger\n",
    "logging.basicConfig(            # setting up the root logger\n",
    "    format='[%(levelname)-5s][%(asctime)s][%(module)s:%(lineno)04d] : %(message)s',\n",
    "    level=logging.INFO,\n",
    "    stream=sys.stdout\n",
    ")\n",
    "logger: logging.Logger = logging.getLogger('root')      # alias the root logger as `logger`\n",
    "logger.setLevel(logging.DEBUG)                          # programmatically reassign the logging level\n",
    "\n",
    "\n",
    "# **** BIGQUERY CLIENT ****\n",
    "logger.debug(f\"Creating bigquery client\")\n",
    "client = bigquery.Client()\n",
    "\n",
    "logger.info(f\"Setup Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset if needed\n",
    "dataset_id = f\"{PROJECT_NAME}.{DATASET_NAME}\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"US\"\n",
    "dataset = client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "logger.info(f\"Created US Monthly Passengers dataset: {dataset.full_dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load source csv\n",
    "air_df = pd.read_csv(filename)\n",
    "\n",
    "# *** always perform check at the end ***\n",
    "# check schema: contains all expected columns?\n",
    "expected_columns = ['Sum_PASSENGERS','AIRLINE_ID','CARRIER_NAME','ORIGIN','ORIGIN_CITY_NAME','ORIGIN_STATE_ABR','ORIGIN_STATE_NM','ORIGIN_COUNTRY','ORIGIN_COUNTRY_NAME','DEST','DEST_CITY_NAME','DEST_STATE_ABR','DEST_STATE_NM','DEST_COUNTRY','DEST_COUNTRY_NAME','YEAR','MONTH']\n",
    "for col in expected_columns:\n",
    "    assert col in list(air_df.columns), f\"Data file missing required column: {col}\"\n",
    "\n",
    "# assign & remember receipts dataframe\n",
    "air_receipts_df = air_df\n",
    "display(air_receipts_df.head(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load airlines dim\n",
    "\n",
    "# start from the receipts\n",
    "airlines_df = air_receipts_df\n",
    "\n",
    "# set of unique columns to return\n",
    "cols = ['CARRIER_NAME','AIRLINE_ID']\n",
    "# group by unique columns and only select them\n",
    "airlines_df = airlines_df.groupby(cols).all()\n",
    "airlines_df = airlines_df.reset_index().loc[:, cols]\n",
    "# rename columns\n",
    "airlines_df = airlines_df.rename(columns={'CARRIER_NAME': 'carrier_name'})\n",
    "\n",
    "logger.info(f\"airline dim - found {len(airlines_df.index)} rows\")\n",
    "display(airlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "# first define a generic function that returns the md4 hash for\n",
    "# any combination of values\n",
    "def get_hash(value) -> str:\n",
    "    \"\"\"return the md5 hash of all parameters\"\"\"\n",
    "    return md5(value.encode(encoding='utf-16')).hexdigest()\n",
    "\n",
    "\n",
    "logger.info(f\"assigning airline ids: using md5 hash of airline name\")\n",
    "\n",
    "# airline_id = md5 hash of carrier_name\n",
    "airlines_df['airline_id'] = airlines_df['carrier_name'].map(get_hash)\n",
    "# set index by airline_id\n",
    "airlines_df = airlines_df.set_index(keys='airline_id')\n",
    "\n",
    "logger.info(f\"airline ids generated\")\n",
    "display(airlines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_airline_df = pd.read_csv('../data/air_travel/global_airlines.csv', header=0)\n",
    "\n",
    "global_airline_df = global_airline_df.rename(columns={'name': 'carrier_name'})\n",
    "global_airline_df = global_airline_df[['carrier_name','iata','icao','callsign','country']]\n",
    "global_airline_df = global_airline_df.set_index(keys='carrier_name')\n",
    "\n",
    "airlines_final_df = airlines_df.join(global_airline_df, on='carrier_name', how='left')\n",
    "\n",
    "airlines_final_df = airlines_final_df.drop(columns='AIRLINE_ID')\n",
    "display(airlines_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_table(\n",
    "    df: pd.DataFrame, \n",
    "    client: bigquery.Client, \n",
    "    table_name: str, \n",
    "    schema: List[bigquery.SchemaField], \n",
    "    create_disposition: str = 'CREATE_IF_NEEDED', \n",
    "    write_disposition: str = 'WRITE_TRUNCATE'\n",
    "    ) -> None:\n",
    "    \"\"\"load dataframe into bigquery table\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe to load\n",
    "        client (bigquery.Client): bigquery client\n",
    "        table_name (str): full table name including project and dataset id\n",
    "        schema (List[bigquery.SchemaField]): table schema with data types\n",
    "        create_disposition (str, optional): create table disposition. Defaults to 'CREATE_IF_NEEDED'.\n",
    "        write_disposition (str, optional): overwrite table disposition. Defaults to 'WRITE_TRUNCATE'.\n",
    "    \"\"\"\n",
    "    # *** run some checks ***\n",
    "    # test table name to be full table name including project and dataset name. It must contain to dots\n",
    "    assert len(table_name.split('.')) == 3, f\"Table name must be a full bigquery table name including project and dataset id: '{table_name}'\"\n",
    "    # setup bigquery load job:\n",
    "    #  create table if needed, replace rows, define the table schema\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        create_disposition=create_disposition,\n",
    "        write_disposition=write_disposition,\n",
    "        schema=schema\n",
    "    )\n",
    "    logger.info(f\"loading table: '{table_name}'\")\n",
    "    job = client.load_table_from_dataframe(df, destination=table_name, job_config=job_config)\n",
    "    job.result()        # wait for the job to finish\n",
    "    # get the resulting table\n",
    "    table = client.get_table(table_name)\n",
    "    logger.info(f\"loaded {table.num_rows} rows into {table.full_table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table name and schema from our TABLE_METADATA config param\n",
    "table_name = f\"{PROJECT_NAME}.{DATASET_NAME}.{TABLE_METADATA['airlines']['table_name']}\"\n",
    "schema = schema=TABLE_METADATA['airlines']['schema']\n",
    "# load dataframe\n",
    "load_table(airlines_final_df, client, table_name, schema)\n",
    "\n",
    "logger.info(f\"loaded airlines dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load airports dim\n",
    "\n",
    "airports_df = pd.read_csv('../data/air_travel/global_airports.csv', header=0)\n",
    "\n",
    "cols = ['iata','name','city','country','icao','latitude','longitude']\n",
    "\n",
    "airports_df = airports_df.groupby(cols).all()\n",
    "airports_df = airports_df.reset_index().loc[:, cols]\n",
    "\n",
    "# rename columns\n",
    "airports_df = airports_df.rename(columns={'iata': 'airport_id'})\n",
    "# remove duplicates and airports without iata code\n",
    "airports_df = airports_df.drop_duplicates(subset='airport_id')\n",
    "airports_df = airports_df[airports_df.name != '(Duplicate) Playa Samara Airport']\n",
    "\n",
    "airports_df = airports_df.set_index(keys='airport_id')\n",
    "\n",
    "logger.info(f\"airport dim - found {len(airlines_df.index)} rows\")\n",
    "display(airports_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table name and schema from our TABLE_METADATA config param\n",
    "table_name = f\"{PROJECT_NAME}.{DATASET_NAME}.{TABLE_METADATA['airports']['table_name']}\"\n",
    "schema = schema=TABLE_METADATA['airports']['schema']\n",
    "# load dataframe\n",
    "load_table(airports_df, client, table_name, schema)\n",
    "\n",
    "logger.info(f\"loaded airports dim\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
